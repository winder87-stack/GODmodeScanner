# AGGRESSIVE SOLANA CLIENT
# High-performance RPC client with:
# - Multiple endpoint round-robin
# - Request batching (JSON-RPC 2.0)
# - LRU caching
# - User-Agent rotation
# - Intelligent rate limiting
# - Exponential backoff retries

import asyncio
import collections
import time
import random
import hashlib
import logging
from typing import List, Tuple, Dict, Optional, Any, Union

try:
    import uvloop
except ImportError:
    uvloop = None

import httpx
import orjson
import cachetools

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger('AggressiveSolanaClient')


class AggressiveSolanaClient:
    """
    High-performance Solana RPC client with aggressive optimization techniques.
    
    Features:
    - Multiple endpoint round-robin for load distribution
    - Request batching for JSON-RPC 2.0 batch operations
    - LRU caching for cacheable responses
    - User-Agent rotation to avoid detection
    - Intelligent rate limiting with adaptive feedback
    - Exponential backoff for automatic retries
    - HTTP/2 support for multiplexing
    - uvloop for maximum event loop performance
    
    Example:
        client = AggressiveSolanaClient()
        
        # Single request
        result = await client.request("getAccountInfo", [wallet, {"encoding": "base64"}])
        
        # Batch request
        results = await client.batch([
            ("getAccountInfo", [wallet1]),
            ("getAccountInfo", [wallet2])
        ])
    """
    
    def __init__(
        self,
        rpc_endpoints: Optional[List[str]] = None,
        initial_rps: float = 10.0,
        min_rps: float = 1.0,
        max_rps: float = 50.0,
        growth_threshold: int = 50,
        throttle_factor: float = 0.5,
        growth_factor: float = 1.1,
        max_retries: int = 3,
        cache_maxsize: int = 1000,
        verify_ssl: bool = False
    ):
        """
        Initialize the Aggressive Solana Client.
        
        Args:
            rpc_endpoints: List of RPC endpoints (default: common public endpoints)
            initial_rps: Starting requests per second
            min_rps: Absolute minimum speed
            max_rps: Upper bound for growth
            growth_threshold: Consecutive successes before increasing speed
            throttle_factor: Speed reduction on 429 (0.5 = 50% cut)
            growth_factor: Speed increase on growth (1.1 = 10% boost)
            max_retries: Maximum retry attempts with exponential backoff
            cache_maxsize: Maximum cache entries
            verify_ssl: Whether to verify SSL certificates
        """
        # Set uvloop for maximum performance
        if uvloop:
            asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
        
        # Multiple RPC endpoints with round-robin
        self.rpc_endpoints = rpc_endpoints or [
            "https://api.mainnet-beta.solana.com",
            "https://solana-api.projectserum.com",
            "https://rpc.ankr.com/solana",
            "https://solana-mainnet.rpc.extrnode.com",
            "https://rpc.mainnet.noonies.com"
        ]
        self.current_endpoint_index = 0
        
        # LRU Cache
        self.cache = cachetools.LRUCache(maxsize=cache_maxsize)
        self.cache_hits = 0
        self.cache_misses = 0
        
        # Rate limiting state
        self.current_rps = initial_rps
        self.max_rps = max_rps
        self.min_rps = min_rps
        self.growth_threshold = growth_threshold
        self.throttle_factor = throttle_factor
        self.growth_factor = growth_factor
        
        self.request_timestamps = collections.deque(maxlen=100)
        self.last_429_time = 0
        self.consecutive_successes = 0
        self.total_requests = 0
        self.total_429s = 0
        
        # Retry configuration
        self.max_retries = max_retries
        self.verify_ssl = verify_ssl
        
        # User-Agent rotation
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36",
            "Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1"
        ]
        
        # HTTP client with aggressive connection limits
        self.client = httpx.AsyncClient(
            http2=True,
            verify=verify_ssl,
            timeout=httpx.Timeout(30.0),
            limits=httpx.Limits(
                max_connections=500,
                max_keepalive_connections=100,
                keepalive_expiry=30.0
            )
        )
        
        logger.info(f"ðŸš€ AggressiveSolanaClient initialized")
        logger.info(f"   Endpoints: {len(self.rpc_endpoints)} (round-robin)")
        logger.info(f"   Starting RPS: {self.current_rps}")
        logger.info(f"   Range: [{self.min_rps}, {self.max_rps}]")
        logger.info(f"   Max Retries: {self.max_retries}")
        logger.info(f"   Cache Size: {cache_maxsize}")
    
    def _get_next_endpoint(self) -> str:
        """Get the next endpoint in round-robin fashion."""
        endpoint = self.rpc_endpoints[self.current_endpoint_index]
        self.current_endpoint_index = (self.current_endpoint_index + 1) % len(self.rpc_endpoints)
        return endpoint
    
    def _get_random_user_agent(self) -> str:
        """Get a random User-Agent string."""
        return random.choice(self.user_agents)
    
    def _get_cache_key(self, method: str, params: tuple) -> str:
        """Generate a cache key from method and params."""
        # Create a deterministic hash for caching
        key_str = f"{method}:{orjson.dumps(params).decode()}"
        return hashlib.sha256(key_str.encode()).hexdigest()
    
    def _check_cache(self, method: str, params: tuple) -> Optional[Any]:
        """Check if response is cached."""
        # Only cache read operations (not write operations)
        write_methods = {'sendTransaction', 'requestAirdrop', 'signTransaction'}
        if method in write_methods:
            return None
        
        cache_key = self._get_cache_key(method, params)
        if cache_key in self.cache:
            self.cache_hits += 1
            logger.debug(f"ðŸ“¦ Cache HIT: {method}")
            return self.cache[cache_key]
        
        self.cache_misses += 1
        return None
    
    def _cache_response(self, method: str, params: tuple, response: Any):
        """Cache a response."""
        write_methods = {'sendTransaction', 'requestAirdrop', 'signTransaction'}
        if method in write_methods:
            return
        
        cache_key = self._get_cache_key(method, params)
        self.cache[cache_key] = response
    
    async def _apply_rate_limit(self):
        """Apply rate limiting based on current RPS."""
        if self.request_timestamps:
            time_since_last = time.time() - self.request_timestamps[-1]
            required_delay = 1.0 / self.current_rps
            
            if time_since_last < required_delay:
                sleep_time = required_delay - time_since_last
                await asyncio.sleep(sleep_time)
    
    async def _make_request(
        self,
        url: str,
        payload: Union[dict, list],
        user_agent: str,
        retry_count: int = 0
    ) -> httpx.Response:
        """
        Make an HTTP request with exponential backoff retry.
        
        Args:
            url: The RPC endpoint URL
            payload: Request payload (dict or list for batch)
            user_agent: User-Agent header
            retry_count: Current retry attempt
            
        Returns:
            HTTP response
            
        Raises:
            httpx.HTTPStatusError: If max retries exceeded
        """
        try:
            response = await self.client.post(
                url,
                json=payload,
                headers={
                    "Content-Type": "application/json",
                    "User-Agent": user_agent
                }
            )
            return response
            
        except (httpx.HTTPStatusError, httpx.RequestError) as e:
            # Rate limit or network error - retry with exponential backoff
            if retry_count < self.max_retries:
                wait_time = 2 ** retry_count
                logger.warning(
                    f"âš ï¸  Request failed (attempt {retry_count + 1}/{self.max_retries}): {e}"
                    f" - Retrying in {wait_time}s..."
                )
                await asyncio.sleep(wait_time)
                return await self._make_request(url, payload, user_agent, retry_count + 1)
            else:
                logger.error(f"âŒ Max retries exceeded: {e}")
                raise
    
    def _process_feedback(self, response: httpx.Response):
        """
        Process response to update rate limiting state (The Brain).
        
        Args:
            response: HTTP response
        """
        self.total_requests += 1
        
        if response.status_code == 429:
            # Rate limit hit - aggressive throttling
            self.total_429s += 1
            old_rps = self.current_rps
            self.current_rps = max(self.min_rps, self.current_rps * self.throttle_factor)
            self.consecutive_successes = 0
            self.last_429_time = time.time()
            
            logger.warning(
                f"""ðŸš¨ ALERT: 429 HIT! ðŸš¨
   Old RPS: {old_rps:.2f}
   New RPS: {self.current_rps:.2f} ({self.throttle_factor*100:.0f}% reduction)
   Total 429s: {self.total_429s}"""
            )
        
        elif response.status_code == 200:
            # Success - track for adaptive growth
            self.consecutive_successes += 1
            
            if self.consecutive_successes >= self.growth_threshold:
                old_rps = self.current_rps
                self.current_rps = min(self.max_rps, self.current_rps * self.growth_factor)
                self.consecutive_successes = 0
                
                logger.info(
                    f"âœ… SUCCESS: Increasing speed to {self.current_rps:.2f} RPS "
                    f"(was {old_rps:.2f})"
                )
            
            elif self.consecutive_successes % 25 == 0:
                logger.info(
                    f"ðŸ“ˆ Progress: {self.consecutive_successes}/{self.growth_threshold} "
                    f"successes @ {self.current_rps:.2f} RPS"
                )
        
        else:
            # Other errors - minor penalty
            logger.warning(
                f"âš ï¸  Status {response.status_code}: {response.text[:100]}"
            )
            self.consecutive_successes = max(0, self.consecutive_successes - 5)
    
    async def request(
        self,
        method: str,
        params: Optional[list] = None,
        request_id: Optional[int] = None
    ) -> dict:
        """
        Execute a single RPC request.
        
        Args:
            method: RPC method name
            params: RPC parameters
            request_id: Optional request ID
            
        Returns:
            Parsed JSON response
        """
        if params is None:
            params = []
        
        params_tuple = tuple(params) if isinstance(params, list) else (params,)
        
        # Check cache
        cached = self._check_cache(method, params_tuple)
        if cached is not None:
            return cached
        
        # Apply rate limiting
        await self._apply_rate_limit()
        
        # Get endpoint and User-Agent
        url = self._get_next_endpoint()
        user_agent = self._get_random_user_agent()
        
        # Build payload
        start_time = time.time()
        payload = {
            "jsonrpc": "2.0",
            "id": request_id or self.total_requests + 1,
            "method": method,
            "params": params
        }
        
        # Make request with retry
        response = await self._make_request(url, payload, user_agent)
        
        # Record timestamp
        end_time = time.time()
        self.request_timestamps.append(end_time)
        
        # Process feedback
        self._process_feedback(response)
        
        # Parse and cache response
        try:
            result = orjson.loads(response.content)
            self._cache_response(method, params_tuple, result)
            return result
        except orjson.JSONDecodeError:
            logger.error(f"Failed to decode response: {response.content[:200]}")
            raise
    
    async def batch(
        self,
        method_params_list: List[Tuple[str, list]]
    ) -> List[dict]:
        """
        Execute multiple RPC requests in a single batch (JSON-RPC 2.0).
        
        Args:
            method_params_list: List of (method, params) tuples
            
        Returns:
            List of parsed JSON responses
            
        Note:
            Caching is applied to individual requests in the batch.
            If a request is cached, it's not included in the network request.
        """
        # Build batch with cache checks
        batch = []
        cache_results = {}
        indices_to_request = []
        
        for idx, (method, params) in enumerate(method_params_list):
            if params is None:
                params = []
            
            params_tuple = tuple(params) if isinstance(params, list) else (params,)
            
            # Check cache for this item
            cached = self._check_cache(method, params_tuple)
            if cached is not None:
                cache_results[idx] = cached
            else:
                # Add to batch
                batch.append({
                    "jsonrpc": "2.0",
                    "id": len(batch) + 1,
                    "method": method,
                    "params": params
                })
                indices_to_request.append((idx, method, params_tuple))
        
        # If all cached, return immediately
        if not batch:
            logger.debug(f"ðŸ“¦ All {len(method_params_list)} requests cached")
            return [cache_results[idx] for idx in range(len(method_params_list))]
        
        # Apply rate limiting
        await self._apply_rate_limit()
        
        # Get endpoint and User-Agent
        url = self._get_next_endpoint()
        user_agent = self._get_random_user_agent()
        
        # Make batch request with retry
        start_time = time.time()
        response = await self._make_request(url, batch, user_agent)
        end_time = time.time()
        
        # Record timestamp
        self.request_timestamps.append(end_time)
        
        # Process feedback (only count batch as one request for rate limiting)
        self.total_requests += 1
        if response.status_code == 200:
            self.consecutive_successes += 1
        
        # Parse batch response
        try:
            batch_results = orjson.loads(response.content)
        except orjson.JSONDecodeError:
            logger.error(f"Failed to decode batch response: {response.content[:200]}")
            raise
        
        # Combine cached and fresh results
        final_results = []
        request_idx = 0
        
        for idx in range(len(method_params_list)):
            if idx in cache_results:
                final_results.append(cache_results[idx])
            else:
                final_results.append(batch_results[request_idx])
                # Cache this result
                _, method, params_tuple = indices_to_request[request_idx]
                self._cache_response(method, params_tuple, batch_results[request_idx])
                request_idx += 1
        
        logger.debug(f"ðŸ“¦ Batch: {len(batch)} requests, {len(cache_results)} cached")
        return final_results
    
    def get_stats(self) -> dict:
        """Get comprehensive performance statistics."""
        total_lookups = self.cache_hits + self.cache_misses
        cache_hit_rate = (self.cache_hits / total_lookups * 100) if total_lookups > 0 else 0
        
        return {
            "current_rps": self.current_rps,
            "total_requests": self.total_requests,
            "total_429s": self.total_429s,
            "consecutive_successes": self.consecutive_successes,
            "success_rate": (self.total_requests - self.total_429s) / max(1, self.total_requests) * 100,
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "cache_hit_rate": cache_hit_rate,
            "cache_size": len(self.cache),
            "active_endpoints": len(self.rpc_endpoints)
        }
    
    async def close(self):
        """Close the HTTP client."""
        await self.client.aclose()
        logger.info("AggressiveSolanaClient closed")


# ============ DEMO USAGE ============
async def demo_aggressive_client():
    """Demonstrate the Aggressive Solana Client."""
    client = AggressiveSolanaClient(
        initial_rps=10.0,
        max_rps=30.0,
        growth_threshold=20,
        max_retries=3
    )
    
    logger.info("Starting Aggressive Client demo...")
    
    # Test wallets
    test_wallets = [
        "7xKXtg2CW87d97TXJSDpbD5jBkheTqA83TZRuJosgAsU",
        "9WzDXwBbmkg8ZTbNMqUxvQRAyrZzDsGYdLVL9zYtAWWM",
        "TokenkegQfeZyiNwAJbNbGKPFXCWuBvf9Ss623VQ5DA"
    ]
    
    try:
        # Test single requests (with caching)
        logger.info("\nðŸ”¹ Testing single requests with caching...")
        for wallet in test_wallets:
            result = await client.request(
                "getAccountInfo",
                [wallet, {"encoding": "base64"}]
            )
            logger.info(f"âœ… Request for {wallet[:8]}... completed")
        
        # Request same wallet again (should hit cache)
        logger.info("\nðŸ”¹ Testing cache hit...")
        result = await client.request(
            "getAccountInfo",
            [test_wallets[0], {"encoding": "base64"}]
        )
        logger.info("âœ… Request completed (likely from cache)")
        
        # Test batch requests
        logger.info("\nðŸ”¹ Testing batch requests...")
        batch_params = [
            ("getAccountInfo", [wallet, {"encoding": "base64"}])
            for wallet in test_wallets
        ]
        
        results = await client.batch(batch_params)
        logger.info(f"âœ… Batch of {len(results)} requests completed")
        
        # Show stats
        stats = client.get_stats()
        logger.info(f"\nðŸ“Š FINAL STATS:")
        logger.info(f"   Total Requests: {stats['total_requests']}")
        logger.info(f"   Final RPS: {stats['current_rps']:.2f}")
        logger.info(f"   Total 429s: {stats['total_429s']}")
        logger.info(f"   Success Rate: {stats['success_rate']:.2f}%")
        logger.info(f"   Cache Hit Rate: {stats['cache_hit_rate']:.2f}%")
        logger.info(f"   Cache Size: {stats['cache_size']} / 1000")
        logger.info(f"   Active Endpoints: {stats['active_endpoints']}")
    
    except KeyboardInterrupt:
        logger.info("Demo interrupted by user")
    finally:
        await client.close()


if __name__ == "__main__":
    asyncio.run(demo_aggressive_client())
