#!/usr/bin/env python3\n\"\"\"BaseWorker Class - DRY Infrastructure for GODMODESCANNER Workers.\n
This module provides a base class that consolidates common worker functionality,
eliminating code duplication across individual worker implementations.

Inherited Features:\n- Redis connection management with connection pooling\n- Structured logging with structlog\n- Message consumption loop (consume-process-ack)\n- Statistics tracking and periodic reporting\n- Graceful shutdown with signal handling\n
Worker-Specific Implementation:\n- process_batch(messages): Abstract method for custom logic\n- initialize(): Optional worker-specific setup\n- disconnect(): Optional cleanup\n\"\"\"\n\nimport asyncio\nimport json\nimport signal\nimport sys\nimport time\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\nimport traceback\n\nimport structlog\nfrom redis.asyncio import Redis\nfrom redis.asyncio.connection import ConnectionPool\n\n# Add project root to path\nproject_root = Path(__file__).parent.parent.parent\nsys.path.insert(0, str(project_root))\n\nfrom utils.redis_streams_consumer import RedisStreamsConsumer, Message\n\nlogger = structlog.get_logger(__name__)\n\n\n# ============================================================================\n# WORKER CONFIGURATION\n# ============================================================================\n
@dataclass\nclass BaseWorkerConfig:\n    \"\"\"Configuration for base worker behavior.\"\"\"\n    \n    # Redis configuration\n    redis_url: str = \"redis://localhost:6379\"\n    max_connections: int = 20\n    \n    # Stream configuration\n    stream_name: str = \"godmode:transactions\"\n    group_name: str = \"worker-group\"\n    consumer_name: Optional[str] = None\n    \n    # Message processing\n    count_per_read: int = 10\n    block_ms: int = 100\n    batch_timeout_seconds: float = 5.0\n    \n    # Statistics\n    stats_interval_seconds: float = 60.0\n    \n    # Shutdown\n    shutdown_timeout_seconds: float = 30.0\n\n\n# ============================================================================\n# BASE WORKER CLASS\n# ============================================================================\n
class BaseWorker(ABC):\n    \"\"\"Base class for all GODMODESCANNER workers.\n    
    This class implements all common infrastructure, allowing individual workers
    to focus only on their specific processing logic.
    
    Usage:\n        class MyWorker(BaseWorker):\n            AGENT_TYPE = \"my-worker\"\n            \n            async def process_batch(self, messages: List[Message]) -> List[dict]:\n                # Worker-specific logic\n                return results\n        
        worker = MyWorker(worker_id=\"my-worker-1\")\n        asyncio.run(worker.run())\n    \"\"\"\n    \n    # Worker identification (override in subclasses)\n    AGENT_TYPE = \"base-worker\"\n    WORKER_VERSION = \"1.0.0\"\n    \n    def __init__(\n        self,\n        worker_id: str,\n        redis_url: str = \"redis://localhost:6379\",\n        config: Optional[BaseWorkerConfig] = None,\n    ):\n        \"\"\"Initialize base worker with configuration.\n        
        Args:\n            worker_id: Unique identifier for this worker instance\n            redis_url: Redis connection URL\n            config: Optional configuration overrides\n        \"\"\"\n        self.worker_id = worker_id\n        self.redis_url = redis_url\n        self.config = config or BaseWorkerConfig()\n        \n        # Ensure consumer name is set\n        if not self.config.consumer_name:\n            self.config.consumer_name = worker_id\n        \n        # Runtime state\n        self._redis_pool: Optional[ConnectionPool] = None\n        self._redis: Optional[Redis] = None\n        self._consumer: Optional[RedisStreamsConsumer] = None\n        self._running = False\n        self._shutdown_event = asyncio.Event()\n        \n        # Statistics (comprehensive metrics)\n        self.stats = {\n            \"worker_id\": worker_id,\n            \"agent_type\": self.AGENT_TYPE,\n            \"version\": self.WORKER_VERSION,\n            \"messages_consumed\": 0,\n            \"messages_acknowledged\": 0,\n            \"messages_failed\": 0,\n            \"batches_processed\": 0,\n            \"last_message_time\": None,\n            \"uptime_seconds\": 0,\n            \"messages_per_second\": 0.0,\n            \"errors\": 0,\n        }\n        \n        # Timing for stats\n        self._start_time: Optional[float] = None\n        self._last_stats_report: float = 0\n        \n        # Initialize structlog\n        self._init_logging()\n        \n        logger.info(\n            \"worker_initialized\",\n            worker_id=worker_id,\n            agent_type=self.AGENT_TYPE,\n            version=self.WORKER_VERSION,\n            redis_url=redis_url\n        )\n    \n    def _init_logging(self):\n        \"\"\"Initialize structured logging for this worker.\"\"\"\n        # Bind common fields to all log entries\n        structlog.configure(\n            processors=[\n                structlog.contextvars.merge_contextvars,\n                structlog.processors.add_log_level,\n                structlog.processors.TimeStamper(fmt=\"iso\"),\n                structlog.processors.StackInfoRenderer(),\n                structlog.processors.format_exc_info,\n                structlog.dev.ConsoleRenderer()\n                if logger\n                else structlog.processors.JSONRenderer(),\n            ],\n            wrapper_class=structlog.make_filtering_bound_logger(\"INFO\"),\n            cache_logger_on_first_use=True,\n        )\n    \n    async def connect(self):\n        \"\"\"Establish Redis connection and initialize consumer.\"\"\"\n        logger.info(\"connecting_to_redis\", redis_url=self.redis_url)\n        \n        # Create connection pool\n        self._redis_pool = ConnectionPool.from_url(\n            self.redis_url,
            max_connections=self.config.max_connections,\n            decode_responses=True,
        )\n        \n        # Create Redis client\n        self._redis = Redis(connection_pool=self._redis_pool)\n        \n        # Test connection\n        try:\n            await self._redis.ping()\n            logger.info(\"redis_connection_successful\")\n        except Exception as e:\n            logger.error(\"redis_connection_failed\", error=str(e))\n            raise\n        \n        # Create consumer\n        self._consumer = RedisStreamsConsumer(\n            redis_url=self.redis_url,\n            stream_name=self.config.stream_name,\n            group_name=self.config.group_name,\n            consumer_name=self.config.consumer_name,\n        )\n        \n        # Initialize consumer group\n        await self._consumer.initialize()\n        \n        logger.info(\n            \"consumer_initialized\",\n            stream=self.config.stream_name,\n            group=self.config.group_name\n        )\n        \n        # Call worker-specific initialization\n        await self.initialize()\n    \n    async def disconnect(self):\n        \"\"\"Close Redis connections and cleanup.\"\"\"\n        logger.info(\"disconnecting_worker\", worker_id=self.worker_id)\n        \n        # Call worker-specific cleanup\n        try:\n            await self.disconnect()\n        except Exception as e:\n            logger.warning(\n                \"worker_cleanup_warning\",\n                error=str(e)\n            )\n        \n        # Close Redis connections\n        if self._redis:\n            await self._redis.close()\n        \n        if self._redis_pool:\n            await self._redis_pool.disconnect()\n        \n        logger.info(\"worker_disconnected\", worker_id=self.worker_id)\n    \n    async def run(self):\n        \"\"\"Main worker loop - consumes and processes messages.\"\"\"\n        if not self._redis:\n            await self.connect()\n        \n        self._running = True\n        self._start_time = time.time()\n        self._last_stats_report = self._start_time\n        \n        logger.info(\n            \"worker_started\",\n            worker_id=self.worker_id,\n            stream=self.config.stream_name\n        )\n        \n        # Setup signal handlers for graceful shutdown\n        loop = asyncio.get_event_loop()\n        \n        try:\n            while self._running:\n                try:\n                    # Consume messages with timeout\n                    messages = await self._consumer.consume_with_timeout(\n                        count=self.config.count_per_read,\n                        block_ms=self.config.block_ms,\n                        timeout_seconds=self.config.batch_timeout_seconds,\n                    )\n                    \n                    if not messages:\n                        continue\n                    \n                    # Update stats\n                    self.stats[\"messages_consumed\"] += len(messages)\n                    self.stats[\"last_message_time\"] = time.time()\n                    \n                    # Process batch (worker-specific logic)\n                    results = await self.process_batch(messages)\n                    \n                    # Acknowledge messages\n                    for msg in messages:\n                        await self._consumer.acknowledge(msg)\n                    \n                    self.stats[\"messages_acknowledged\"] += len(messages)\n                    \n                    # Update batch stats\n                    self.stats[\"batches_processed\"] += 1\n                    \n                except asyncio.TimeoutError:\n                    # Timeout is expected, continue loop\n                    pass\n                except Exception as e:\n                    logger.error(\n                        \"batch_processing_error\",\n                        worker_id=self.worker_id,\n                        error=str(e),\n                        traceback=traceback.format_exc()\n                    )\n                    self.stats[\"errors\"] += 1\n                    self.stats[\"messages_failed\"] += len(messages) if 'messages' in dir() else 0\n                \n                # Periodic stats reporting\n                await self._maybe_report_stats()\n                \n        except asyncio.CancelledError:\n            logger.info(\"worker_cancelled\", worker_id=self.worker_id)\n        finally:\n            self._running = False\n            await self._graceful_shutdown()\n    \n    async def _maybe_report_stats(self):\n        \"\"\"Report statistics at configured interval.\"\"\"\n        current_time = time.time()\n        \n        if current_time - self._last_stats_report >= self.config.stats_interval_seconds:\n            self._last_stats_report = current_time\n            \n            # Calculate derived metrics\n            uptime = current_time - self._start_time\n            self.stats[\"uptime_seconds\"] = uptime\n            \n            if uptime > 0:\n                self.stats[\"messages_per_second\"] = (\n                    self.stats[\"messages_consumed\"] / uptime\n                )\n            \n            logger.info(\"worker_stats\", **self.stats)\n    \n    async def _graceful_shutdown(self):\n        \"\"\"Perform graceful shutdown with timeout.\"\"\"\n        logger.info(\n            \"initiating_shutdown\",\n            worker_id=self.worker_id,\n            timeout=self.config.shutdown_timeout_seconds\n        )\n        \n        # Wait for shutdown with timeout\n        try:\n            await asyncio.wait_for(\n                self._shutdown_event.wait(),\n                timeout=self.config.shutdown_timeout_seconds\n            )\n        except asyncio.TimeoutError:\n            logger.warning(\"shutdown_timeout\", worker_id=self.worker_id)\n        \n        await self.disconnect()\n        \n        logger.info(\n            \"worker_stopped\",\n            worker_id=self.worker_id,\n            final_stats=self.stats\n        )\n    \n    def stop(self):\n        \"\"\Signal worker to stop gracefully.\"\"\"\n        logger.info(\"stop_signal_received\", worker_id=self.worker_id)\n        self._running = False\n        self._shutdown_event.set()\n    \n    # =========================================================================\n    # ABSTRACT METHODS (implement in subclasses)\n    # =========================================================================\n    
    @abstractmethod\n    async def process_batch(self, messages: List[Message]) -> List[dict]:\n        \"\"\"Process a batch of messages.\n        
        This is the main worker-specific implementation.
        All common infrastructure is handled by BaseWorker.
        
        Args:\n            messages: List of messages from Redis Stream\n            
        Returns:\n            List of processing results\n        \"\"\"\n        raise NotImplementedError(\"process_batch must be implemented by subclass\")\n    \n    async def initialize(self):\n        \"\"\"Optional worker-specific initialization.\n        
        Override this method for worker-specific setup:
        - Creating additional connections\n        - Loading models\n        - Setting up state\n        \"\"\"\n        pass\n    \n    async def disconnect(self):\n        \"\"\"Optional worker-specific cleanup.\n        
        Override this method for worker-specific cleanup:
        - Closing additional connections\n        - Saving state\n        - Cleanup resources\n        \"\"\"\n        pass\n    \n    # =========================================================================\n    # PUBLIC METHODS\n    # =========================================================================\n    
    async def health_check(self) -> Dict[str, Any]:\n        \"\"\"Check worker health and return status.\"\"\"\n        return {\n            \"healthy\": self._running,\n            \"worker_id\": self.worker_id,\n            \"agent_type\": self.AGENT_TYPE,\n            \"version\": self.WORKER_VERSION,\n            \"uptime_seconds\": self.stats[\"uptime_seconds\"],\n            \"messages_per_second\": round(self.stats[\"messages_per_second\"], 2),\n        }\n    \n    async def get_stats(self) -> Dict[str, Any]:\n        \"\"\Return current statistics (returns copy).\"\"\"\n        # Update uptime before returning\n        if self._start_time:\n            self.stats[\"uptime_seconds\"] = time.time() - self._start_time\n        return dict(self.stats)\n    \n    def __repr__(self) -> str:\n        return (\n            f\"{self.__class__.__name__}(\"\n            f\"worker_id={self.worker_id!r}, \"\n            f\"redis_url={self.redis_url!r}, \"\n            f\"agent_type={self.AGENT_TYPE!r}\"\n            f\")\"\n        )\n\n
